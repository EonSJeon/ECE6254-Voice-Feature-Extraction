{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsooSNaODOh6Vz/cFrL3UC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oS2s2kQes52s","executionInfo":{"status":"ok","timestamp":1744310664291,"user_tz":240,"elapsed":130325,"user":{"displayName":"Sangeon Jeon","userId":"15757893315633779349"}},"outputId":"faceecc8-034e-451b-97ab-4e701029837b"},"outputs":[{"output_type":"stream","name":"stdout","text":["KNN accuracy: 0.607\n","Decision Tree accuracy: 0.455\n","SVM accuracy: 0.584\n","Random Forest accuracy: 0.5655\n","Gradient Boosting accuracy: 0.493\n","AdaBoost accuracy: 0.417\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:44:18] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost accuracy: 0.562\n","Logistic Regression accuracy: 0.4415\n","\n","Best model on validation set: KNN\n","Test accuracy for best model (KNN): 0.5634\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import ast\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","import xgboost as xgb\n","from sklearn.linear_model import LogisticRegression\n","\n","# 병합된 CSV 파일 경로 (원본 메타데이터와 feature가 결합된 파일)\n","train_csv_path = \"df_balanced_updated.csv\"\n","valid_csv_path = \"df_valid_updated.csv\"\n","test_csv_path  = \"df_test_updated.csv\"\n","\n","# --- Load Merged Data ---\n","df_train = pd.read_csv(train_csv_path)\n","df_valid = pd.read_csv(valid_csv_path)\n","df_test  = pd.read_csv(test_csv_path)\n","\n","# --- Helper Function: 문자열 벡터 파싱 ---\n","def parse_vector(x):\n","    if not isinstance(x, str):\n","        return x\n","    x = x.strip()\n","    if x.startswith('[') and x.endswith(']'):\n","        try:\n","            # 우선 ast.literal_eval 시도\n","            return ast.literal_eval(x)\n","        except Exception:\n","            # 파싱 실패 시, 대괄호 제거 후 공백 기준으로 분리하여 float 리스트로 변환\n","            x = x[1:-1].strip()  # 앞뒤 대괄호 제거\n","            tokens = x.split()   # 공백(및 개행) 기준 분리\n","            try:\n","                return [float(token) for token in tokens if token]\n","            except Exception:\n","                return None\n","    return x\n","\n","# --- Preprocessing: 벡터 형태의 컬럼 확장하기 ---\n","def expand_vector_columns(df, vector_cols):\n","    for col in vector_cols:\n","        # 각 셀에 대해 parse_vector 적용\n","        df[col] = df[col].apply(parse_vector)\n","        # 리스트 형태라면, 각 원소를 개별 컬럼으로 확장 (리스트 길이가 모두 동일하다고 가정)\n","        vector_df = pd.DataFrame(df[col].tolist(), index=df.index)\n","        vector_df = vector_df.add_prefix(col + '_')\n","        df = df.drop(col, axis=1).join(vector_df)\n","    return df\n","\n","# 확장할 컬럼 리스트 (필요에 따라 수정 가능)\n","vector_columns = ['mfcc_mean', 'mfcc_std', 'mfcc_delta_mean', 'mfcc_delta_std']\n","df_train = expand_vector_columns(df_train, vector_columns)\n","df_valid = expand_vector_columns(df_valid, vector_columns)\n","df_test  = expand_vector_columns(df_test, vector_columns)\n","\n","# --- Prepare Target and Features ---\n","target_col = 'accent_encoded'\n","# 학습에 사용하지 않을 메타데이터 컬럼 지정 (target 포함)\n","exclude_cols = ['filename', 'gender', 'accent', 'age', target_col]\n","# 전체 컬럼 중에서 제외할 컬럼을 제거하여 특성 컬럼 선택\n","feature_columns = [col for col in df_train.columns if col not in exclude_cols]\n","\n","# X, y 배열로 추출\n","X_train = df_train[feature_columns].values\n","y_train = df_train[target_col].values\n","\n","X_valid = df_valid[feature_columns].values\n","y_valid = df_valid[target_col].values\n","\n","X_test = df_test[feature_columns].values\n","y_test = df_test[target_col].values\n","\n","# --- Scale Features ---\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_valid_scaled = scaler.transform(X_valid)\n","X_test_scaled  = scaler.transform(X_test)\n","\n","# --- Model Training and Validation ---\n","model_performance = {}\n","\n","# 1) K-Nearest Neighbors (KNN)\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train_scaled, y_train)\n","y_pred_knn = knn.predict(X_valid_scaled)\n","acc_knn = accuracy_score(y_valid, y_pred_knn)\n","model_performance['KNN'] = acc_knn\n","print(\"KNN accuracy:\", acc_knn)\n","\n","# 2) Decision Tree\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train_scaled, y_train)\n","y_pred_dt = dt.predict(X_valid_scaled)\n","acc_dt = accuracy_score(y_valid, y_pred_dt)\n","model_performance['Decision Tree'] = acc_dt\n","print(\"Decision Tree accuracy:\", acc_dt)\n","\n","# 3) Support Vector Machine (SVM)\n","svm = SVC(kernel='rbf', C=1.0, random_state=42)\n","svm.fit(X_train_scaled, y_train)\n","y_pred_svm = svm.predict(X_valid_scaled)\n","acc_svm = accuracy_score(y_valid, y_pred_svm)\n","model_performance['SVM'] = acc_svm\n","print(\"SVM accuracy:\", acc_svm)\n","\n","# 4) Random Forest\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train_scaled, y_train)\n","y_pred_rf = rf.predict(X_valid_scaled)\n","acc_rf = accuracy_score(y_valid, y_pred_rf)\n","model_performance['Random Forest'] = acc_rf\n","print(\"Random Forest accuracy:\", acc_rf)\n","\n","# 5) Gradient Boosting\n","gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n","gb.fit(X_train_scaled, y_train)\n","y_pred_gb = gb.predict(X_valid_scaled)\n","acc_gb = accuracy_score(y_valid, y_pred_gb)\n","model_performance['Gradient Boosting'] = acc_gb\n","print(\"Gradient Boosting accuracy:\", acc_gb)\n","\n","# 6) AdaBoost\n","ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n","ada.fit(X_train_scaled, y_train)\n","y_pred_ada = ada.predict(X_valid_scaled)\n","acc_ada = accuracy_score(y_valid, y_pred_ada)\n","model_performance['AdaBoost'] = acc_ada\n","print(\"AdaBoost accuracy:\", acc_ada)\n","\n","# 7) XGBoost\n","xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1,\n","                            random_state=42, use_label_encoder=False,\n","                            eval_metric='mlogloss')\n","xgb_clf.fit(X_train_scaled, y_train)\n","y_pred_xgb = xgb_clf.predict(X_valid_scaled)\n","acc_xgb = accuracy_score(y_valid, y_pred_xgb)\n","model_performance['XGBoost'] = acc_xgb\n","print(\"XGBoost accuracy:\", acc_xgb)\n","\n","# 8) Logistic Regression\n","lr = LogisticRegression(max_iter=1000, random_state=42)\n","lr.fit(X_train_scaled, y_train)\n","y_pred_lr = lr.predict(X_valid_scaled)\n","acc_lr = accuracy_score(y_valid, y_pred_lr)\n","model_performance['Logistic Regression'] = acc_lr\n","print(\"Logistic Regression accuracy:\", acc_lr)\n","\n","# --- Determine the Best Model on Validation Set and Evaluate on Test Set ---\n","best_model_name = max(model_performance, key=model_performance.get)\n","print(\"\\nBest model on validation set:\", best_model_name)\n","\n","if best_model_name == 'KNN':\n","    best_model = knn\n","elif best_model_name == 'Decision Tree':\n","    best_model = dt\n","elif best_model_name == 'SVM':\n","    best_model = svm\n","elif best_model_name == 'Random Forest':\n","    best_model = rf\n","elif best_model_name == 'Gradient Boosting':\n","    best_model = gb\n","elif best_model_name == 'AdaBoost':\n","    best_model = ada\n","elif best_model_name == 'XGBoost':\n","    best_model = xgb_clf\n","elif best_model_name == 'Logistic Regression':\n","    best_model = lr\n","\n","y_test_pred = best_model.predict(X_test_scaled)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","print(\"Test accuracy for best model ({}): {:.4f}\".format(best_model_name, acc_test))\n"]}]}