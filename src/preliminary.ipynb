{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-existence filtered accent counts:\n",
      "accent\n",
      "us           29777\n",
      "england      14648\n",
      "indian        4382\n",
      "australia     4020\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === df_train + df_valid ===\n",
    "BASE_DIR = '/Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/'\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, 'raw_data')\n",
    "TRAIN_CSV_PATH = os.path.join(RAW_DATA_DIR, 'cv-valid-train.csv')\n",
    "\n",
    "accents = ['us', 'england', 'indian', 'australia']\n",
    "ages = ['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties']\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Drop rows with missing gender\n",
    "df_train = df_train.dropna(subset=['gender'])\n",
    "\n",
    "# Filter accents of interest\n",
    "df_train = df_train[df_train['accent'].isin(accents)]\n",
    "\n",
    "# Drop rows with missing accent\n",
    "df_train = df_train.dropna(subset=['accent'])\n",
    "\n",
    "# Check if each file exists in RAW_DATA_DIR and remove rows where file is missing\n",
    "df_train = df_train[df_train['filename'].apply(lambda x: os.path.exists(os.path.join(RAW_DATA_DIR, x)))]\n",
    "\n",
    "\n",
    "df_train = df_train[['filename', 'gender', 'accent', 'age']]\n",
    "\n",
    "print(\"File-existence filtered accent counts:\")\n",
    "print(df_train['accent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-existence filtered accent counts for test set:\n",
      "accent\n",
      "us           626\n",
      "england      298\n",
      "indian        90\n",
      "australia     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === df_test ===\n",
    "TEST_CSV_PATH = os.path.join(RAW_DATA_DIR, 'cv-valid-test.csv')\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "# Drop rows with missing gender\n",
    "df_test = df_test.dropna(subset=['gender'])\n",
    "\n",
    "# Filter accents of interest\n",
    "df_test = df_test[df_test['accent'].isin(accents)]\n",
    "\n",
    "# Drop rows with missing accent\n",
    "df_test = df_test.dropna(subset=['accent'])\n",
    "\n",
    "# Check if each file exists in RAW_DATA_DIR and remove rows where file is missing\n",
    "df_test = df_test[df_test['filename'].apply(lambda x: os.path.exists(os.path.join(RAW_DATA_DIR, x)))]\n",
    "\n",
    "df_test = df_test[['filename', 'gender', 'accent', 'age']]\n",
    "\n",
    "print(\"File-existence filtered accent counts for test set:\")\n",
    "print(df_test['accent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resample factor information saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/train_dataset_info.txt\n"
     ]
    }
   ],
   "source": [
    "# Work on a copy of the training data\n",
    "df_temp = df_train.copy()\n",
    "\n",
    "# Get all unique accents and count them\n",
    "all_accents = df_temp['accent'].unique()\n",
    "num_accents = len(all_accents)\n",
    "\n",
    "# Identify (gender, age) combinations that are present in every accent\n",
    "combo_counts = df_temp.groupby(['gender', 'age'])['accent'].nunique()\n",
    "valid_combos = combo_counts[combo_counts == num_accents].index.tolist()\n",
    "\n",
    "balanced_groups = []\n",
    "resample_info_lines = []  # List to store resample factor info\n",
    "\n",
    "# Process each valid (gender, age) combination\n",
    "for gender, age in valid_combos:\n",
    "    # Subset data for the current (gender, age) cell\n",
    "    subset = df_temp[(df_temp['gender'] == gender) & (df_temp['age'] == age)]\n",
    "    \n",
    "    # Compute available counts per accent for this cell\n",
    "    accent_counts = subset.groupby('accent').size()\n",
    "    \n",
    "    # Determine the target T: cannot exceed 1.5 times the minimum count and must be at most the maximum available count.\n",
    "    T = min(accent_counts.max(), int(1.5 * accent_counts.min()))\n",
    "    \n",
    "    # For each accent, sample T records: oversample (with replacement) if needed or undersample otherwise.\n",
    "    for accent, group in subset.groupby('accent'):\n",
    "        current_count = len(group)\n",
    "        factor = T / current_count\n",
    "        if current_count < T:\n",
    "            method = 'oversampled'\n",
    "            sampled = group.sample(n=T, replace=True, random_state=42)\n",
    "        else:\n",
    "            method = 'undersampled'\n",
    "            sampled = group.sample(n=T, replace=False, random_state=42)\n",
    "        balanced_groups.append(sampled)\n",
    "        \n",
    "        # Record resample factor info for this group\n",
    "        info_line = (f\"Gender: {gender}, Age: {age}, Accent: {accent}, \"\n",
    "                     f\"Original count: {current_count}, Target count: {T}, \"\n",
    "                     f\"Factor: {factor:.2f} ({method})\")\n",
    "        resample_info_lines.append(info_line)\n",
    "\n",
    "\n",
    "df_balanced = pd.concat(balanced_groups, ignore_index=True)\n",
    "\n",
    "\n",
    "# Save resample factor information to a text file\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")  # Using absolute path\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "resample_txt_path = os.path.join(output_dir, 'train_dataset_info.txt')\n",
    "with open(resample_txt_path, \"w\") as f:\n",
    "    f.write(\"Resample Factor Information:\\n\")\n",
    "    for line in resample_info_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "print(\"Resample factor information saved to\", resample_txt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples per accent:\n",
      "accent\n",
      "australia    2724\n",
      "england      2724\n",
      "indian       2724\n",
      "us           2724\n",
      "dtype: int64\n",
      "Balanced Accent x Age Confusion Matrix:\n",
      "age        fifties  fourties  sixties  teens  thirties  twenties\n",
      "accent                                                          \n",
      "australia      665        67       73    198       769       952\n",
      "england        665        67       73    198       769       952\n",
      "indian         665        67       73    198       769       952\n",
      "us             665        67       73    198       769       952\n",
      "\n",
      "Balanced Accent x Gender Confusion Matrix:\n",
      "gender     female  male\n",
      "accent                 \n",
      "australia    1052  1672\n",
      "england      1052  1672\n",
      "indian       1052  1672\n",
      "us           1052  1672\n",
      "\n",
      "Total number of records in balanced dataset: 10896\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of samples per accent to verify they are equal\n",
    "accent_totals = df_balanced.groupby('accent').size()\n",
    "print(\"\\nTotal samples per accent:\")\n",
    "print(accent_totals)\n",
    "\n",
    "# Display the balanced Accent x Age confusion matrix\n",
    "confusion_matrix_age = pd.crosstab(df_balanced['accent'], df_balanced['age'])\n",
    "print(\"Balanced Accent x Age Confusion Matrix:\")\n",
    "print(confusion_matrix_age)\n",
    "\n",
    "# Display the balanced Accent x Gender confusion matrix\n",
    "confusion_matrix_gender = pd.crosstab(df_balanced['accent'], df_balanced['gender'])\n",
    "print(\"\\nBalanced Accent x Gender Confusion Matrix:\")\n",
    "print(confusion_matrix_gender)\n",
    "\n",
    "print(f\"\\nTotal number of records in balanced dataset: {len(df_balanced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accent counts in validation set:\n",
      "accent\n",
      "australia    500\n",
      "england      500\n",
      "indian       500\n",
      "us           500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exclude rows that are in df_balanced from df_train\n",
    "df_valid_candidate = df_train[~df_train['filename'].isin(df_balanced['filename'])]\n",
    "\n",
    "balanced_valid_groups = []\n",
    "# For each accent, sample exactly 500 records to balance the accent distribution.\n",
    "# (If a group has less than 500 records, it will sample all available records.)\n",
    "for accent, group in df_valid_candidate.groupby('accent'):\n",
    "    n_samples = 500 if len(group) >= 500 else len(group)\n",
    "    sampled_group = group.sample(n=n_samples, random_state=42)\n",
    "    balanced_valid_groups.append(sampled_group)\n",
    "\n",
    "# Combine the groups to create the balanced validation DataFrame\n",
    "df_valid = pd.concat(balanced_valid_groups, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Print the number of samples per accent in the validation set\n",
    "accent_counts = df_valid['accent'].value_counts()\n",
    "print(\"Accent counts in validation set:\")\n",
    "print(accent_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset accent counts:\n",
      "accent_encoded\n",
      "0    2724\n",
      "1    2724\n",
      "2    2724\n",
      "3    2724\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation dataset accent counts:\n",
      "accent_encoded\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label classes:\n",
      "['australia' 'england' 'indian' 'us']\n",
      "Accent label mapping appended to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/label_mapping_info.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Create a label encoder and fit on the 'accent' column of df_balanced\n",
    "label_encoder = LabelEncoder()\n",
    "df_balanced['accent_encoded'] = label_encoder.fit_transform(df_balanced['accent'])\n",
    "\n",
    "# Use the same label encoder to transform the 'accent' column of df_valid\n",
    "df_valid['accent_encoded'] = label_encoder.transform(df_valid['accent'])\n",
    "\n",
    "df_test['accent_encoded'] = label_encoder.transform(df_test['accent'])\n",
    "\n",
    "# Print counts for verification\n",
    "print(\"Balanced dataset accent counts:\")\n",
    "print(df_balanced['accent_encoded'].value_counts())\n",
    "print(\"\\nValidation dataset accent counts:\")\n",
    "print(df_valid['accent_encoded'].value_counts())\n",
    "print(\"\\nLabel classes:\")\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Append the accent label mapping information to the existing info text file\n",
    "txt_out_path = os.path.join(BASE_DIR, 'data', 'label_mapping_info.txt')\n",
    "with open(txt_out_path, \"a\") as f:  # open in append mode\n",
    "    f.write(\"\\nAccent Label Mapping:\\n\")\n",
    "    for encoded_value, accent in enumerate(label_encoder.classes_):\n",
    "        f.write(f\"{encoded_value}: {accent}\\n\")\n",
    "\n",
    "print(\"Accent label mapping appended to\", txt_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced validation DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_valid.csv\n",
      "Balanced DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_train_balanced.csv\n",
      "Test DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the balanced validation DataFrame to a CSV file\n",
    "df_valid_csv_path = os.path.join(output_dir, 'df_valid.csv')\n",
    "df_valid.to_csv(df_valid_csv_path, index=False)\n",
    "print(\"Balanced validation DataFrame saved to\", df_valid_csv_path)\n",
    "\n",
    "# Save the balanced DataFrame to a CSV file in the same directory\n",
    "df_balanced_csv_path = os.path.join(output_dir, 'df_train_balanced.csv')\n",
    "df_balanced.to_csv(df_balanced_csv_path, index=False)\n",
    "print(\"Balanced DataFrame saved to\", df_balanced_csv_path)\n",
    "\n",
    "# Save the df_test DataFrame to a CSV file\n",
    "output_dir = os.path.join(BASE_DIR, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df_test_csv_path = os.path.join(output_dir, 'df_test.csv')\n",
    "df_test.to_csv(df_test_csv_path, index=False)\n",
    "print(\"Test DataFrame saved to\", df_test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_balance하고 df_valid, df_test하고 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCCs for training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train MFCCs: 100%|██████████| 10896/10896 [02:20<00:00, 77.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCCs for validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid MFCCs: 100%|██████████| 2000/2000 [00:25<00:00, 78.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCCs for test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test MFCCs: 100%|██████████| 1104/1104 [00:14<00:00, 76.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure RAW_DATA_DIR is defined (the folder containing your audio files)\n",
    "# For example:\n",
    "# RAW_DATA_DIR = '/path/to/your/audio/files'\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=20):\n",
    "    audio, sr = librosa.load(file_path)\n",
    "    win_length = int(0.025 * sr)   # 25ms window length\n",
    "    hop_length = int(0.01 * sr)    # 10ms hop length\n",
    "    n_fft = win_length           # Using window length as n_fft (can be adjusted)\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window='hann'\n",
    "    )\n",
    "    return mfccs\n",
    "\n",
    "# === Process Training Data (df_balanced) ===\n",
    "all_mfccs_train = []\n",
    "print(\"Extracting MFCCs for training set:\")\n",
    "for idx, row in tqdm(df_balanced.iterrows(), total=len(df_balanced), desc=\"Train MFCCs\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    mfcc = extract_mfcc(path)\n",
    "    all_mfccs_train.append(mfcc)\n",
    "\n",
    "# === Process Validation Data (df_valid) ===\n",
    "all_mfccs_valid = []\n",
    "print(\"Extracting MFCCs for validation set:\")\n",
    "for idx, row in tqdm(df_valid.iterrows(), total=len(df_valid), desc=\"Valid MFCCs\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    mfcc = extract_mfcc(path)\n",
    "    all_mfccs_valid.append(mfcc)\n",
    "\n",
    "# === Process Test Data (df_test) ===\n",
    "all_mfccs_test = []\n",
    "print(\"Extracting MFCCs for test set:\")\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Test MFCCs\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    mfcc = extract_mfcc(path)\n",
    "    all_mfccs_test.append(mfcc)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median time frames (train+valid): 381\n"
     ]
    }
   ],
   "source": [
    "# Compute the median number of time frames across both training and validation MFCCs\n",
    "time_frames_train = [mfcc.shape[1] for mfcc in all_mfccs_train]\n",
    "time_frames_valid = [mfcc.shape[1] for mfcc in all_mfccs_valid]\n",
    "time_frames_test = [mfcc.shape[1] for mfcc in all_mfccs_test]\n",
    "all_time_frames = time_frames_train + time_frames_valid + time_frames_test\n",
    "median_time_frames = int(np.median(all_time_frames))\n",
    "print(\"Median time frames (train+valid):\", median_time_frames)\n",
    "\n",
    "# Pad or truncate MFCCs to have the same number of time frames (equal to the median)\n",
    "def pad_or_truncate(mfcc, target_length):\n",
    "    if mfcc.shape[1] < target_length:\n",
    "        padded = np.pad(mfcc, ((0, 0), (0, target_length - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded = mfcc[:, :target_length]\n",
    "    return padded\n",
    "\n",
    "padded_mfccs_train = [pad_or_truncate(mfcc, median_time_frames) for mfcc in all_mfccs_train]\n",
    "padded_mfccs_valid = [pad_or_truncate(mfcc, median_time_frames) for mfcc in all_mfccs_valid]\n",
    "padded_mfccs_test = [pad_or_truncate(mfcc, median_time_frames) for mfcc in all_mfccs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training dataset to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/train-dataset.npz\n",
      "Saved validation dataset to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/valid-dataset.npz\n",
      "Saved test dataset to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/test-dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Create X arrays and one-hot encode labels\n",
    "X_train = np.stack(padded_mfccs_train)\n",
    "X_valid = np.stack(padded_mfccs_valid)\n",
    "X_test = np.stack(padded_mfccs_test)\n",
    "y_train = to_categorical(df_balanced['accent_encoded'].values)\n",
    "y_valid = to_categorical(df_valid['accent_encoded'].values)\n",
    "y_test = to_categorical(df_test['accent_encoded'].values)\n",
    "\n",
    "# Save each dataset to an NPZ file\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_npz_path = os.path.join(output_dir, \"train-dataset.npz\")\n",
    "np.savez(train_npz_path, X=X_train, y=y_train)\n",
    "print(\"Saved training dataset to\", train_npz_path)\n",
    "\n",
    "valid_npz_path = os.path.join(output_dir, \"valid-dataset.npz\")\n",
    "np.savez(valid_npz_path, X=X_valid, y=y_valid)\n",
    "print(\"Saved validation dataset to\", valid_npz_path)\n",
    "\n",
    "test_npz_path = os.path.join(output_dir, \"test-dataset.npz\")\n",
    "np.savez(test_npz_path, X=X_test, y=y_test)\n",
    "print(\"Saved test dataset to\", test_npz_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
