{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-existence filtered accent counts:\n",
      "accent\n",
      "us           29777\n",
      "england      14648\n",
      "indian        4382\n",
      "australia     4020\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === df_train + df_valid ===\n",
    "BASE_DIR = '/Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/'\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, 'raw_data')\n",
    "TRAIN_CSV_PATH = os.path.join(RAW_DATA_DIR, 'cv-valid-train.csv')\n",
    "\n",
    "accents = ['us' ,'england', 'indian', 'australia']\n",
    "ages = ['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties']\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Drop rows with missing gender\n",
    "df_train = df_train.dropna(subset=['gender'])\n",
    "\n",
    "# Filter accents of interest\n",
    "df_train = df_train[df_train['accent'].isin(accents)]\n",
    "\n",
    "# Drop rows with missing accent\n",
    "df_train = df_train.dropna(subset=['accent'])\n",
    "\n",
    "# Check if each file exists in RAW_DATA_DIR and remove rows where file is missing\n",
    "df_train = df_train[df_train['filename'].apply(lambda x: os.path.exists(os.path.join(RAW_DATA_DIR, x)))]\n",
    "\n",
    "\n",
    "df_train = df_train[['filename', 'gender', 'accent', 'age']]\n",
    "\n",
    "print(\"File-existence filtered accent counts:\")\n",
    "print(df_train['accent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-existence filtered accent counts for test set:\n",
      "accent\n",
      "us           626\n",
      "england      298\n",
      "indian        90\n",
      "australia     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === df_test ===\n",
    "TEST_CSV_PATH = os.path.join(RAW_DATA_DIR, 'cv-valid-test.csv')\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "# Drop rows with missing gender\n",
    "df_test = df_test.dropna(subset=['gender'])\n",
    "\n",
    "# Filter accents of interest\n",
    "df_test = df_test[df_test['accent'].isin(accents)]\n",
    "\n",
    "# Drop rows with missing accent\n",
    "df_test = df_test.dropna(subset=['accent'])\n",
    "\n",
    "# Check if each file exists in RAW_DATA_DIR and remove rows where file is missing\n",
    "df_test = df_test[df_test['filename'].apply(lambda x: os.path.exists(os.path.join(RAW_DATA_DIR, x)))]\n",
    "\n",
    "df_test = df_test[['filename', 'gender', 'accent', 'age']]\n",
    "\n",
    "print(\"File-existence filtered accent counts for test set:\")\n",
    "print(df_test['accent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resample factor information saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/train_dataset_info.txt\n"
     ]
    }
   ],
   "source": [
    "# Work on a copy of the training data\n",
    "df_temp = df_train.copy()\n",
    "\n",
    "# Get all unique accents and count them\n",
    "all_accents = df_temp['accent'].unique()\n",
    "num_accents = len(all_accents)\n",
    "\n",
    "# Identify (gender, age) combinations that are present in every accent\n",
    "combo_counts = df_temp.groupby(['gender', 'age'])['accent'].nunique()\n",
    "valid_combos = combo_counts[combo_counts == num_accents].index.tolist()\n",
    "\n",
    "balanced_groups = []\n",
    "resample_info_lines = []  # List to store resample factor info\n",
    "\n",
    "# Process each valid (gender, age) combination\n",
    "for gender, age in valid_combos:\n",
    "    # Subset data for the current (gender, age) cell\n",
    "    subset = df_temp[(df_temp['gender'] == gender) & (df_temp['age'] == age)]\n",
    "    \n",
    "    # Compute available counts per accent for this cell\n",
    "    accent_counts = subset.groupby('accent').size()\n",
    "    \n",
    "    # Determine the target T: cannot exceed 1.5 times the minimum count and must be at most the maximum available count.\n",
    "    T = min(accent_counts.max(), int(1.5 * accent_counts.min()))\n",
    "    \n",
    "    # For each accent, sample T records: oversample (with replacement) if needed or undersample otherwise.\n",
    "    for accent, group in subset.groupby('accent'):\n",
    "        current_count = len(group)\n",
    "        factor = T / current_count\n",
    "        if current_count < T:\n",
    "            method = 'oversampled'\n",
    "            sampled = group.sample(n=T, replace=True, random_state=42)\n",
    "        else:\n",
    "            method = 'undersampled'\n",
    "            sampled = group.sample(n=T, replace=False, random_state=42)\n",
    "        balanced_groups.append(sampled)\n",
    "        \n",
    "        # Record resample factor info for this group\n",
    "        info_line = (f\"Gender: {gender}, Age: {age}, Accent: {accent}, \"\n",
    "                     f\"Original count: {current_count}, Target count: {T}, \"\n",
    "                     f\"Factor: {factor:.2f} ({method})\")\n",
    "        resample_info_lines.append(info_line)\n",
    "\n",
    "\n",
    "df_balanced = pd.concat(balanced_groups, ignore_index=True)\n",
    "\n",
    "\n",
    "# Save resample factor information to a text file\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")  # Using absolute path\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "resample_txt_path = os.path.join(output_dir, 'train_dataset_info.txt')\n",
    "with open(resample_txt_path, \"w\") as f:\n",
    "    f.write(\"Resample Factor Information:\\n\")\n",
    "    for line in resample_info_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "print(\"Resample factor information saved to\", resample_txt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples per accent:\n",
      "accent\n",
      "australia    2724\n",
      "england      2724\n",
      "indian       2724\n",
      "us           2724\n",
      "dtype: int64\n",
      "Balanced Accent x Age Confusion Matrix:\n",
      "age        fifties  fourties  sixties  teens  thirties  twenties\n",
      "accent                                                          \n",
      "australia      665        67       73    198       769       952\n",
      "england        665        67       73    198       769       952\n",
      "indian         665        67       73    198       769       952\n",
      "us             665        67       73    198       769       952\n",
      "\n",
      "Balanced Accent x Gender Confusion Matrix:\n",
      "gender     female  male\n",
      "accent                 \n",
      "australia    1052  1672\n",
      "england      1052  1672\n",
      "indian       1052  1672\n",
      "us           1052  1672\n",
      "\n",
      "Total number of records in balanced dataset: 10896\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of samples per accent to verify they are equal\n",
    "accent_totals = df_balanced.groupby('accent').size()\n",
    "print(\"\\nTotal samples per accent:\")\n",
    "print(accent_totals)\n",
    "\n",
    "# Display the balanced Accent x Age confusion matrix\n",
    "confusion_matrix_age = pd.crosstab(df_balanced['accent'], df_balanced['age'])\n",
    "print(\"Balanced Accent x Age Confusion Matrix:\")\n",
    "print(confusion_matrix_age)\n",
    "\n",
    "# Display the balanced Accent x Gender confusion matrix\n",
    "confusion_matrix_gender = pd.crosstab(df_balanced['accent'], df_balanced['gender'])\n",
    "print(\"\\nBalanced Accent x Gender Confusion Matrix:\")\n",
    "print(confusion_matrix_gender)\n",
    "\n",
    "print(f\"\\nTotal number of records in balanced dataset: {len(df_balanced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accent counts in validation set:\n",
      "accent\n",
      "australia    500\n",
      "england      500\n",
      "indian       500\n",
      "us           500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exclude rows that are in df_balanced from df_train\n",
    "df_valid_candidate = df_train[~df_train['filename'].isin(df_balanced['filename'])]\n",
    "\n",
    "balanced_valid_groups = []\n",
    "# For each accent, sample exactly 500 records to balance the accent distribution.\n",
    "# (If a group has less than 500 records, it will sample all available records.)\n",
    "for accent, group in df_valid_candidate.groupby('accent'):\n",
    "    n_samples = 500 if len(group) >= 500 else len(group)\n",
    "    sampled_group = group.sample(n=n_samples, random_state=42)\n",
    "    balanced_valid_groups.append(sampled_group)\n",
    "\n",
    "# Combine the groups to create the balanced validation DataFrame\n",
    "df_valid = pd.concat(balanced_valid_groups, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Print the number of samples per accent in the validation set\n",
    "accent_counts = df_valid['accent'].value_counts()\n",
    "print(\"Accent counts in validation set:\")\n",
    "print(accent_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset accent counts:\n",
      "accent_encoded\n",
      "0    2724\n",
      "1    2724\n",
      "2    2724\n",
      "3    2724\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation dataset accent counts:\n",
      "accent_encoded\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label classes:\n",
      "['australia' 'england' 'indian' 'us']\n",
      "Accent label mapping appended to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/label_mapping_info.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Create a label encoder and fit on the 'accent' column of df_balanced\n",
    "label_encoder = LabelEncoder()\n",
    "df_balanced['accent_encoded'] = label_encoder.fit_transform(df_balanced['accent'])\n",
    "\n",
    "# Use the same label encoder to transform the 'accent' column of df_valid\n",
    "df_valid['accent_encoded'] = label_encoder.transform(df_valid['accent'])\n",
    "\n",
    "df_test['accent_encoded'] = label_encoder.transform(df_test['accent'])\n",
    "\n",
    "# Print counts for verification\n",
    "print(\"Balanced dataset accent counts:\")\n",
    "print(df_balanced['accent_encoded'].value_counts())\n",
    "print(\"\\nValidation dataset accent counts:\")\n",
    "print(df_valid['accent_encoded'].value_counts())\n",
    "print(\"\\nLabel classes:\")\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Append the accent label mapping information to the existing info text file\n",
    "txt_out_path = os.path.join(BASE_DIR, 'data', 'label_mapping_info.txt')\n",
    "with open(txt_out_path, \"a\") as f:  # open in append mode\n",
    "    f.write(\"\\nAccent Label Mapping:\\n\")\n",
    "    for encoded_value, accent in enumerate(label_encoder.classes_):\n",
    "        f.write(f\"{encoded_value}: {accent}\\n\")\n",
    "\n",
    "print(\"Accent label mapping appended to\", txt_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced validation DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_valid.csv\n",
      "Balanced DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_train_balanced.csv\n",
      "Test DataFrame saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the balanced validation DataFrame to a CSV file\n",
    "df_valid_csv_path = os.path.join(output_dir, 'df_valid.csv')\n",
    "df_valid.to_csv(df_valid_csv_path, index=False)\n",
    "print(\"Balanced validation DataFrame saved to\", df_valid_csv_path)\n",
    "\n",
    "# Save the balanced DataFrame to a CSV file in the same directory\n",
    "df_balanced_csv_path = os.path.join(output_dir, 'df_train_balanced.csv')\n",
    "df_balanced.to_csv(df_balanced_csv_path, index=False)\n",
    "print(\"Balanced DataFrame saved to\", df_balanced_csv_path)\n",
    "\n",
    "# Save the df_test DataFrame to a CSV file\n",
    "output_dir = os.path.join(BASE_DIR, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df_test_csv_path = os.path.join(output_dir, 'df_test.csv')\n",
    "df_test.to_csv(df_test_csv_path, index=False)\n",
    "print(\"Test DataFrame saved to\", df_test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed df_balance, df_valid, and df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutlifeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train features: 100%|██████████| 10896/10896 [39:33<00:00,  4.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid features: 100%|██████████| 2000/2000 [07:20<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test features: 100%|██████████| 1104/1104 [03:46<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training features to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/train_features.csv\n",
      "Saved validation features to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/valid_features.csv\n",
      "Saved test features to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import gc\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def lowpass_filter(data, sr, cutoff=4000, order=5):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth low-pass filter to the data.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "def extract_global_features(file_path, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract a concise set of global (file-level) features for accent classification:\n",
    "\n",
    "    1. Energy (sum of squares)\n",
    "    2. RMS Energy\n",
    "    3. Zero-Crossing Rate\n",
    "    4. MFCC (mean & std across time)\n",
    "    5. Delta MFCC (mean & std)\n",
    "    6. Fundamental Frequency (mean, std)\n",
    "    7. Jitter (based on F0 contour)\n",
    "    8. (Optional) Spectral Centroid & Bandwidth (mean)\n",
    "    \"\"\"\n",
    "    # Load the audio file (mono).\n",
    "    audio, sr = librosa.load(file_path)\n",
    "    \n",
    "    # Optional: Low-pass filter to remove frequencies above 5000 Hz\n",
    "    audio = lowpass_filter(audio, sr, cutoff=5000)\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    # 1. Energy: sum of squares\n",
    "    features['energy'] = np.sum(audio**2)\n",
    "    \n",
    "    # 2. RMS Energy\n",
    "    features['rms'] = np.sqrt(np.mean(audio**2))\n",
    "    \n",
    "    # 3. Zero-Crossing Rate (total count)\n",
    "    zcr = np.sum(librosa.zero_crossings(audio, pad=False))\n",
    "    features['zero_crossings'] = zcr\n",
    "    \n",
    "    # 4. MFCC (mean & std across time)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    features['mfcc_mean'] = np.mean(mfcc, axis=1)  # shape: (n_mfcc,)\n",
    "    features['mfcc_std']  = np.std(mfcc, axis=1)\n",
    "    \n",
    "    # 5. Delta MFCC (mean & std)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    features['mfcc_delta_mean'] = np.mean(mfcc_delta, axis=1)\n",
    "    features['mfcc_delta_std']  = np.std(mfcc_delta, axis=1)\n",
    "    \n",
    "    # 6. Fundamental Frequency (using librosa.yin)\n",
    "    #    We'll compute mean & std of F0\n",
    "    f0 = librosa.yin(y=audio, fmin=80, fmax=300)\n",
    "    features['f0_mean'] = np.nanmean(f0)\n",
    "    features['f0_std']  = np.nanstd(f0)\n",
    "    \n",
    "    # 7. Jitter Feature: relative mean absolute difference of consecutive F0 values\n",
    "    diff_f0 = np.abs(np.diff(f0))\n",
    "    mean_f0 = np.nanmean(f0)\n",
    "    features['jitter'] = np.nanmean(diff_f0) / mean_f0 if mean_f0 > 0 else np.nan\n",
    "    \n",
    "    # 8. (Optional) Spectral Centroid & Bandwidth (mean)\n",
    "    #    Comment out if you want fewer features / faster extraction\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    features['spectral_centroid_mean'] = np.mean(spec_centroid)\n",
    "\n",
    "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    features['spectral_bandwidth_mean'] = np.mean(spec_bandwidth)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# === Process Training Data (df_balanced) ===\n",
    "all_features_train = []\n",
    "print(\"Extracting features for training set:\")\n",
    "for idx, row in tqdm(df_balanced.iterrows(), total=len(df_balanced), desc=\"Train features\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    feats = extract_global_features(path, n_mfcc=13)\n",
    "    all_features_train.append(feats)\n",
    "\n",
    "# === Process Validation Data (df_valid) ===\n",
    "all_features_valid = []\n",
    "print(\"Extracting features for validation set:\")\n",
    "for idx, row in tqdm(df_valid.iterrows(), total=len(df_valid), desc=\"Valid features\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    feats = extract_global_features(path, n_mfcc=13)\n",
    "    all_features_valid.append(feats)\n",
    "\n",
    "# === Process Test Data (df_test) ===\n",
    "all_features_test = []\n",
    "print(\"Extracting features for test set:\")\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Test features\"):\n",
    "    path = os.path.join(RAW_DATA_DIR, row['filename'])\n",
    "    feats = extract_global_features(path, n_mfcc=13)\n",
    "    all_features_test.append(feats)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# --- Save extracted features to CSV files ---\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df_train_features = pd.DataFrame(all_features_train)\n",
    "df_valid_features = pd.DataFrame(all_features_valid)\n",
    "df_test_features  = pd.DataFrame(all_features_test)\n",
    "\n",
    "train_csv_path = os.path.join(output_dir, \"train_features.csv\")\n",
    "df_train_features.to_csv(train_csv_path, index=False)\n",
    "print(\"Saved training features to\", train_csv_path)\n",
    "\n",
    "valid_csv_path = os.path.join(output_dir, \"valid_features.csv\")\n",
    "df_valid_features.to_csv(valid_csv_path, index=False)\n",
    "print(\"Saved validation features to\", valid_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(output_dir, \"test_features.csv\")\n",
    "df_test_features.to_csv(test_csv_path, index=False)\n",
    "print(\"Saved test features to\", test_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated df_balanced (first 5 rows):\n",
      "                           filename  gender     accent      age  \\\n",
      "0  cv-valid-train/sample-033345.mp3  female  australia  fifties   \n",
      "1  cv-valid-train/sample-140829.mp3  female  australia  fifties   \n",
      "2  cv-valid-train/sample-086038.mp3  female  australia  fifties   \n",
      "3  cv-valid-train/sample-033921.mp3  female  australia  fifties   \n",
      "4  cv-valid-train/sample-026004.mp3  female  australia  fifties   \n",
      "\n",
      "   accent_encoded      energy       rms  zero_crossings  \\\n",
      "0               0   33.239445  0.024938            6092   \n",
      "1               0  145.872191  0.034694            3281   \n",
      "2               0  176.136347  0.044118           10197   \n",
      "3               0   25.483028  0.013635            3743   \n",
      "4               0  660.516930  0.079196           16344   \n",
      "\n",
      "                                           mfcc_mean  \\\n",
      "0  [-537.9019644079377, 94.94824728793137, -4.160...   \n",
      "1  [-519.6272294839237, 116.2643277268801, 50.740...   \n",
      "2  [-388.4322792946102, 119.98319873362333, -16.6...   \n",
      "3  [-635.1887456907644, 86.2905616064379, 36.4372...   \n",
      "4  [-422.4328788587811, 45.311146285171894, -3.07...   \n",
      "\n",
      "                                            mfcc_std  \\\n",
      "0  [118.94888636425023, 79.80179951651331, 26.447...   \n",
      "1  [69.67492161733034, 58.78649431343403, 27.6366...   \n",
      "2  [118.81555951453694, 54.67365599612318, 55.805...   \n",
      "3  [63.190445550513395, 53.1094820686147, 19.1437...   \n",
      "4  [144.4740924397123, 67.10218215841232, 26.3396...   \n",
      "\n",
      "                                     mfcc_delta_mean  \\\n",
      "0  [-0.0595503727131438, -0.22816073520170926, -0...   \n",
      "1  [-0.165264188170261, 0.0520090635970294, 0.241...   \n",
      "2  [0.18599364638918867, 0.2786882913335556, -0.1...   \n",
      "3  [0.08952929971617894, 0.09593121811389087, 0.0...   \n",
      "4  [-0.00552157016610011, -0.006865489738531038, ...   \n",
      "\n",
      "                                      mfcc_delta_std     f0_mean     f0_std  \\\n",
      "0  [16.46171167102033, 9.258505648186127, 6.11491...  179.475824  50.477490   \n",
      "1  [9.645328697465454, 7.00044906033259, 5.443598...  186.545980  72.961530   \n",
      "2  [18.45480904095627, 11.530848211525814, 9.4666...  187.820603  42.146403   \n",
      "3  [7.8640426719135474, 6.673976670762736, 3.1734...  119.281409  56.858500   \n",
      "4  [19.430970089806305, 10.82615023348524, 5.6119...  190.472763  70.335209   \n",
      "\n",
      "     jitter  spectral_centroid_mean  spectral_bandwidth_mean  \n",
      "0  0.133665             1573.689047              1261.591100  \n",
      "1  0.074787              714.843847               871.017016  \n",
      "2  0.124700             1683.456197              1328.639656  \n",
      "3  0.056437             1186.932616              1423.742160  \n",
      "4  0.162223             2176.288871              1569.039764  \n",
      "\n",
      "Updated df_valid (first 5 rows):\n",
      "                           filename  gender     accent       age  \\\n",
      "0  cv-valid-train/sample-074887.mp3    male  australia   fifties   \n",
      "1  cv-valid-train/sample-136078.mp3    male  australia   fifties   \n",
      "2  cv-valid-train/sample-127559.mp3    male  australia  twenties   \n",
      "3  cv-valid-train/sample-037066.mp3  female  australia       NaN   \n",
      "4  cv-valid-train/sample-136882.mp3    male  australia   sixties   \n",
      "\n",
      "   accent_encoded     energy       rms  zero_crossings  \\\n",
      "0               0   9.317468  0.014308            5154   \n",
      "1               0  12.026865  0.011495           11179   \n",
      "2               0   8.583087  0.007736           16476   \n",
      "3               0   8.713009  0.012124            4335   \n",
      "4               0   6.753741  0.008903           10629   \n",
      "\n",
      "                                           mfcc_mean  \\\n",
      "0  [-551.9012016993646, 120.83611025507429, -27.5...   \n",
      "1  [-603.7958870640009, 120.0632145587354, 9.4218...   \n",
      "2  [-660.5608181706738, 81.88524226371933, 1.8200...   \n",
      "3  [-521.120083833083, 126.88756886048037, -36.27...   \n",
      "4  [-609.7609411333738, 90.26061008401523, -3.336...   \n",
      "\n",
      "                                            mfcc_std  \\\n",
      "0  [145.96874354807125, 85.88038975398501, 38.808...   \n",
      "1  [138.10357323671556, 88.51931247238322, 40.499...   \n",
      "2  [122.09661564082042, 69.60273205298006, 37.438...   \n",
      "3  [115.79123082181921, 40.73090084064581, 31.815...   \n",
      "4  [153.78671691334446, 77.95974056110052, 24.161...   \n",
      "\n",
      "                                     mfcc_delta_mean  \\\n",
      "0  [1.1129828138961357, 0.47417830320071735, 0.06...   \n",
      "1  [-0.06905364363102683, -0.0229255918704651, 0....   \n",
      "2  [-0.14762623415252385, -0.04689978167323979, -...   \n",
      "3  [2.267436006684037, 1.1427109921633922, -0.177...   \n",
      "4  [0.04045044562954439, 0.04685374396646597, 0.0...   \n",
      "\n",
      "                                      mfcc_delta_std     f0_mean     f0_std  \\\n",
      "0  [16.074001723128255, 11.160130579801898, 7.355...  141.492631  55.898606   \n",
      "1  [18.142802487931174, 13.59520981136842, 8.5548...  124.298555  42.723383   \n",
      "2  [13.597772534665364, 9.615334824862448, 7.6682...  142.104050  42.860482   \n",
      "3  [15.272224112143979, 5.933063693339542, 4.6477...  169.964713  48.561037   \n",
      "4  [15.0098687998325, 9.627460852437522, 3.898899...  124.515359  47.701285   \n",
      "\n",
      "     jitter  spectral_centroid_mean  spectral_bandwidth_mean  \n",
      "0  0.134337             1574.471831              1267.782537  \n",
      "1  0.188712             1435.899324              1143.915560  \n",
      "2  0.104151             1606.560712              1390.564705  \n",
      "3  0.138444             1568.632961              1521.031221  \n",
      "4  0.159775             1803.744524              1549.718187  \n",
      "\n",
      "Updated df_test (first 5 rows):\n",
      "                          filename gender   accent       age  accent_encoded  \\\n",
      "0  cv-valid-test/sample-000003.mp3   male       us  twenties               3   \n",
      "1  cv-valid-test/sample-000005.mp3   male       us  twenties               3   \n",
      "2  cv-valid-test/sample-000008.mp3   male  england  thirties               1   \n",
      "3  cv-valid-test/sample-000009.mp3   male       us   fifties               3   \n",
      "4  cv-valid-test/sample-000018.mp3   male  england  fourties               1   \n",
      "\n",
      "        energy       rms  zero_crossings  \\\n",
      "0    39.473606  0.015244           11970   \n",
      "1   418.388865  0.063838            9147   \n",
      "2  1733.323640  0.112023           20219   \n",
      "3   201.027690  0.045314           12387   \n",
      "4    73.666238  0.033107            9635   \n",
      "\n",
      "                                           mfcc_mean  \\\n",
      "0  [-564.7824953785035, 114.02519157879509, 1.128...   \n",
      "1  [-364.13871536520685, 124.23050480360321, -12....   \n",
      "2  [-273.56787826415774, 122.23569376137895, -11....   \n",
      "3  [-373.26941613631635, 127.58248852744418, -56....   \n",
      "4  [-476.55183839635976, 73.37686581630197, -42.0...   \n",
      "\n",
      "                                            mfcc_std  \\\n",
      "0  [120.14940807422947, 72.39448453847368, 40.399...   \n",
      "1  [101.0419600929649, 58.43475064008602, 32.5583...   \n",
      "2  [110.9976894111715, 78.79736873087181, 35.6471...   \n",
      "3  [136.1695157971096, 67.72551722578618, 44.4092...   \n",
      "4  [172.92336633259544, 69.80762579024928, 54.698...   \n",
      "\n",
      "                                     mfcc_delta_mean  \\\n",
      "0  [0.1356773110552907, 0.16389949056285955, 0.11...   \n",
      "1  [0.09981674856588561, 0.0022006026474338143, -...   \n",
      "2  [0.3303004633482174, -0.426070638022616, -0.05...   \n",
      "3  [0.4260453436183864, -0.27556994275693986, -0....   \n",
      "4  [0.017197217992590066, 0.05891174487628779, 0....   \n",
      "\n",
      "                                      mfcc_delta_std     f0_mean     f0_std  \\\n",
      "0  [14.743865795848949, 10.392857368157935, 7.016...  147.641277  49.060016   \n",
      "1  [13.229243598579478, 8.599335438012233, 5.9272...  131.872922  29.943882   \n",
      "2  [18.200956264870324, 15.81446109985638, 5.8808...  160.150111  54.093541   \n",
      "3  [18.724872152898605, 9.24163571628521, 7.84230...  157.239927  52.123948   \n",
      "4  [22.03622730587137, 10.385933544819121, 9.7577...  190.412907  52.268473   \n",
      "\n",
      "     jitter  spectral_centroid_mean  spectral_bandwidth_mean  \n",
      "0  0.139181             1192.847894              1138.889829  \n",
      "1  0.104762             1421.109423              1249.625429  \n",
      "2  0.192066             1832.788964              1309.715401  \n",
      "3  0.106159             1647.591513              1196.390181  \n",
      "4  0.114103             1978.303433              1349.071104  \n",
      "Updated balanced data saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_balanced_updated.csv\n",
      "Updated valid data saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_valid_updated.csv\n",
      "Updated test data saved to /Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/data/df_test_updated.csv\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터프레임에 추출된 feature들을 열 단위로 병합합니다.\n",
    "df_balanced_updated = pd.concat([df_balanced.reset_index(drop=True), df_train_features.reset_index(drop=True)], axis=1)\n",
    "df_valid_updated    = pd.concat([df_valid.reset_index(drop=True), df_valid_features.reset_index(drop=True)], axis=1)\n",
    "df_test_updated     = pd.concat([df_test.reset_index(drop=True), df_test_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 병합된 데이터프레임의 앞부분 일부를 출력해서 제대로 병합되었는지 확인해봅니다.\n",
    "print(\"Updated df_balanced (first 5 rows):\")\n",
    "print(df_balanced_updated.head())\n",
    "\n",
    "print(\"\\nUpdated df_valid (first 5 rows):\")\n",
    "print(df_valid_updated.head())\n",
    "\n",
    "print(\"\\nUpdated df_test (first 5 rows):\")\n",
    "print(df_test_updated.head())\n",
    "\n",
    "# 필요하다면, 병합한 결과를 CSV 파일로 저장할 수 있습니다.\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")\n",
    "balanced_csv_path = os.path.join(output_dir, \"df_balanced_updated.csv\")\n",
    "df_balanced_updated.to_csv(balanced_csv_path, index=False)\n",
    "print(\"Updated balanced data saved to\", balanced_csv_path)\n",
    "\n",
    "valid_csv_path = os.path.join(output_dir, \"df_valid_updated.csv\")\n",
    "df_valid_updated.to_csv(valid_csv_path, index=False)\n",
    "print(\"Updated valid data saved to\", valid_csv_path)\n",
    "\n",
    "test_csv_path = os.path.join(output_dir, \"df_test_updated.csv\")\n",
    "df_test_updated.to_csv(test_csv_path, index=False)\n",
    "print(\"Updated test data saved to\", test_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
