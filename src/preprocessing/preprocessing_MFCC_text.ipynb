{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) Audio Preprocessing (MFCC) ==========\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def lowpass_filter(data, sr, cutoff=4000, order=5):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth low-pass filter to the data.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=20, cutoff=5000):\n",
    "    \"\"\"\n",
    "    Load audio, apply low-pass filter, and extract MFCC spectrogram.\n",
    "    Returns a 2D numpy array of shape (n_mfcc, time_frames).\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=None)  # sr=None -> original sampling rate\n",
    "    # 1) Optional low-pass filter\n",
    "    audio = lowpass_filter(audio, sr, cutoff=cutoff)\n",
    "    \n",
    "    # 2) 파라미터 설정 (예: 25ms window, 10ms hop)\n",
    "    win_length = int(0.025 * sr)\n",
    "    hop_length = int(0.01 * sr)\n",
    "    n_fft = win_length\n",
    "    \n",
    "    # 3) MFCC 추출\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window='hann'\n",
    "    )\n",
    "    return mfcc  # shape: (n_mfcc, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2) Text Preprocessing (Tokenization) ==========\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 사전학습된 모델의 이름\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def tokenize_text(text, max_length=32):\n",
    "    \"\"\"\n",
    "    BERT tokenizer를 이용해 text를 토큰화하고,\n",
    "    최대 길이를 넘어가면 잘라내고, 짧으면 패딩.\n",
    "    반환값: (input_ids, attention_mask)\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='np'  # numpy 배열로 반환\n",
    "    )\n",
    "    input_ids = encoding['input_ids'][0]         # shape: (max_length,)\n",
    "    attention_mask = encoding['attention_mask'][0]  # shape: (max_length,)\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3) Padding/Truncation for MFCC ==========\n",
    "def pad_or_truncate_mfcc(mfcc, target_time_frames):\n",
    "    \"\"\"\n",
    "    mfcc: shape (n_mfcc, T)\n",
    "    target_time_frames: 패딩/잘라낼 T 길이\n",
    "    \"\"\"\n",
    "    n_mfcc, current_T = mfcc.shape\n",
    "    \n",
    "    if current_T < target_time_frames:\n",
    "        # Zero-padding\n",
    "        padded = np.pad(mfcc, ((0, 0), (0, target_time_frames - current_T)), mode='constant')\n",
    "    else:\n",
    "        # Truncate\n",
    "        padded = mfcc[:, :target_time_frames]\n",
    "    \n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "BASE_DIR = '/Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/'\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, 'raw_data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'data')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 관심있는 액센트와 연령대를 정의\n",
    "ACCENTS_OF_INTEREST = ['us', 'england', 'indian', 'australia']\n",
    "AGES_OF_INTEREST = ['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties']\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1) CSV 로드 및 필터링 함수\n",
    "# --------------------------------------------------------------------------------\n",
    "def load_and_filter_csv(csv_path, accents=ACCENTS_OF_INTEREST):\n",
    "    \"\"\"\n",
    "    1) CSV 로드\n",
    "    2) gender, accent 결측 제거\n",
    "    3) 관심 액센트만 선택\n",
    "    4) 실제 오디오 파일이 존재하는 행만 남김\n",
    "    5) 필요한 컬럼만 반환\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # gender, accent 결측 제거\n",
    "    df = df.dropna(subset=['gender', 'accent'])\n",
    "    \n",
    "    # 관심 액센트만 남김\n",
    "    df = df[df['accent'].isin(accents)]\n",
    "    \n",
    "    # 실제 파일이 존재하는지 확인\n",
    "    df = df[df['filename'].apply(lambda x: os.path.exists(os.path.join(RAW_DATA_DIR, x)))]\n",
    "\n",
    "    # 필요한 컬럼만 선택 (filename, text, gender, accent, age)\n",
    "    keep_cols = ['filename', 'text', 'gender', 'accent', 'age']\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2) (Train 전용) (gender, age)별로 액센트 수를 맞추기 위한 Over/Under 샘플링\n",
    "# --------------------------------------------------------------------------------\n",
    "def balance_by_gender_age(df):\n",
    "    \"\"\"\n",
    "    (gender, age) 조합별로, 모든 액센트가 동일한 개수가 되도록\n",
    "    오버샘플링/언더샘플링을 적용.\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    all_accents = df_temp['accent'].unique()\n",
    "    num_accents = len(all_accents)\n",
    "\n",
    "    # (gender, age) 조합별로 액센트가 모두 존재하는지 확인\n",
    "    combo_counts = df_temp.groupby(['gender', 'age'])['accent'].nunique()\n",
    "    valid_combos = combo_counts[combo_counts == num_accents].index.tolist()\n",
    "\n",
    "    balanced_groups = []\n",
    "    resample_info = []\n",
    "\n",
    "    for gender, age in valid_combos:\n",
    "        subset = df_temp[(df_temp['gender'] == gender) & (df_temp['age'] == age)]\n",
    "        \n",
    "        # 해당 (gender, age)에서 액센트별 개수 확인\n",
    "        accent_counts = subset.groupby('accent').size()\n",
    "        \n",
    "        # min 개수 대비 1.5배 이하 & max 개수 이하로 T 결정\n",
    "        T = min(accent_counts.max(), int(1.5 * accent_counts.min()))\n",
    "        \n",
    "        for accent_val, group in subset.groupby('accent'):\n",
    "            current_count = len(group)\n",
    "            factor = T / current_count\n",
    "            if current_count < T:\n",
    "                # Over-sampling\n",
    "                sampled = group.sample(n=T, replace=True, random_state=42)\n",
    "                method = 'oversampled'\n",
    "            else:\n",
    "                # Under-sampling\n",
    "                sampled = group.sample(n=T, replace=False, random_state=42)\n",
    "                method = 'undersampled'\n",
    "            \n",
    "            balanced_groups.append(sampled)\n",
    "            \n",
    "            info_line = (f\"[{gender}, {age}, {accent_val}]  \"\n",
    "                         f\"Original={current_count}, Target={T}, \"\n",
    "                         f\"Factor={factor:.2f}, Method={method}\")\n",
    "            resample_info.append(info_line)\n",
    "\n",
    "    df_balanced = pd.concat(balanced_groups, ignore_index=True)\n",
    "\n",
    "    return df_balanced, resample_info\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3) (Train 전용) 검증 세트(valid) 분리\n",
    "# --------------------------------------------------------------------------------\n",
    "def split_train_valid_by_accent(df_balanced, df_train_original, valid_size=500):\n",
    "    \"\"\"\n",
    "    df_balanced (이미 oversample/undersample 된 train)\n",
    "    df_train_original (원본 train 필터링 결과)\n",
    "    \n",
    "    1) df_balanced에 포함되지 않은 row들만 candidate로 가져옴\n",
    "    2) 각 accent별로 최대 valid_size개 샘플링\n",
    "    3) df_valid 생성 후 반환\n",
    "    \"\"\"\n",
    "    # df_balanced에 없는 파일만 후보로\n",
    "    df_candidates = df_train_original[~df_train_original['filename'].isin(df_balanced['filename'])]\n",
    "\n",
    "    valid_groups = []\n",
    "    for accent, group in df_candidates.groupby('accent'):\n",
    "        n_samples = min(valid_size, len(group))\n",
    "        sampled_group = group.sample(n=n_samples, random_state=42)\n",
    "        valid_groups.append(sampled_group)\n",
    "\n",
    "    df_valid = pd.concat(valid_groups, ignore_index=True)\n",
    "    return df_valid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4) Main Function to create NPZ ==========\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_npz_from_csv(\n",
    "    csv_path,\n",
    "    audio_base_dir,\n",
    "    output_npz_path,\n",
    "    n_mfcc=13,\n",
    "    cutoff=5000,\n",
    "    max_length_text=32,\n",
    "    time_frames_target=None,\n",
    "    label_col='accent'\n",
    "):\n",
    "    \"\"\"\n",
    "    1) CSV 로드 -> 2) 오디오(MFCC) & 텍스트(token) 추출 -> 3) 라벨 인코딩 -> 4) npz 저장\n",
    "    csv_path: CSV 파일 경로 (filename, text, accent 등이 포함)\n",
    "    audio_base_dir: 오디오가 저장된 상위 디렉토리\n",
    "    output_npz_path: 최종 npz가 저장될 경로\n",
    "    n_mfcc: MFCC 개수\n",
    "    cutoff: 저역통과 필터 커트오프 주파수\n",
    "    max_length_text: 텍스트 토큰 최대 길이\n",
    "    time_frames_target: MFCC 시간 프레임 수 (None이면 median으로 자동 결정)\n",
    "    label_col: 타겟 라벨 컬럼명 (예: 'accent')\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1) 결측치/조건 filtering (필요하다면 추가)\n",
    "    # 여기서는 간단히 filename과 text가 결측이면 제외\n",
    "    df = df.dropna(subset=['filename', 'text'])\n",
    "    \n",
    "    # 2) 라벨 인코딩 (accent)\n",
    "    le = LabelEncoder()\n",
    "    df[label_col] = df[label_col].fillna('unknown')  # 혹시 결측 accent 있으면 임시 처리\n",
    "    df['label_encoded'] = le.fit_transform(df[label_col])\n",
    "    \n",
    "    # 3) 오디오 및 텍스트 전처리\n",
    "    mfcc_list = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    print(f\"Extracting MFCC & tokens from {len(df)} samples...\")\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        audio_path = os.path.join(audio_base_dir, row['filename'])\n",
    "        # (A) 오디오 -> MFCC\n",
    "        mfcc = extract_mfcc(audio_path, n_mfcc=n_mfcc, cutoff=cutoff)\n",
    "        mfcc_list.append(mfcc)\n",
    "        \n",
    "        # (B) 텍스트 -> 토큰\n",
    "        txt = str(row['text'])  # 텍스트\n",
    "        input_ids, att_mask = tokenize_text(txt, max_length=max_length_text)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(att_mask)\n",
    "    \n",
    "    # 4) MFCC 시간 길이 기준 결정 (time_frames_target이 None이면 median 산출)\n",
    "    if time_frames_target is None:\n",
    "        all_time_frames = [mf.shape[1] for mf in mfcc_list]\n",
    "        time_frames_target = int(np.median(all_time_frames))\n",
    "        print(\"Auto-selected median time frames:\", time_frames_target)\n",
    "    else:\n",
    "        print(\"Using user-defined time frames:\", time_frames_target)\n",
    "    \n",
    "    # 5) MFCC 리스트를 패딩/자르기하여 고정 길이로 맞춤\n",
    "    padded_mfcc_list = [pad_or_truncate_mfcc(m, time_frames_target) for m in mfcc_list]\n",
    "    \n",
    "    # 6) 넘파이 배열로 변환 (채널 차원 추가: (n, 1, n_mfcc, time_frames))\n",
    "    X_audio = np.stack([np.expand_dims(m, axis=0) for m in padded_mfcc_list])\n",
    "    \n",
    "    # 텍스트 토큰들 (n, max_length)\n",
    "    X_input_ids = np.stack(input_ids_list)\n",
    "    X_attention_mask = np.stack(attention_mask_list)\n",
    "    \n",
    "    # 라벨 (원-핫 인코딩 예시)\n",
    "    labels = df['label_encoded'].values\n",
    "    y = to_categorical(labels)  # (n, num_classes)\n",
    "    \n",
    "    # 7) NPZ로 저장\n",
    "    np.savez_compressed(\n",
    "        output_npz_path,\n",
    "        X_audio=X_audio,\n",
    "        X_input_ids=X_input_ids,\n",
    "        X_attention_mask=X_attention_mask,\n",
    "        y=y,\n",
    "        label_encoder_classes=le.classes_,\n",
    "        time_frames=time_frames_target  # 메타 정보\n",
    "    )\n",
    "    print(f\"Saved dataset to {output_npz_path}\")\n",
    "    print(f\"Shape of X_audio: {X_audio.shape}\")\n",
    "    print(f\"Shape of X_input_ids: {X_input_ids.shape}, X_attention_mask: {X_attention_mask.shape}\")\n",
    "    print(f\"Shape of y (one-hot): {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC & tokens from 145133 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 104/145133 [00:09<3:40:38, 10.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 1) train 세트 변환\u001b[39;00m\n\u001b[32m     16\u001b[39m train_npz_path = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mtrain_dataset.npz\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mcreate_npz_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_CSV_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio_base_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_npz_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_npz_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length_text\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_frames_target\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# median을 자동으로 결정\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 2) valid 세트 변환\u001b[39;00m\n\u001b[32m     29\u001b[39m valid_npz_path = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mvalid_dataset.npz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mcreate_npz_from_csv\u001b[39m\u001b[34m(csv_path, audio_base_dir, output_npz_path, n_mfcc, cutoff, max_length_text, time_frames_target, label_col)\u001b[39m\n\u001b[32m     44\u001b[39m audio_path = os.path.join(audio_base_dir, row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# (A) 오디오 -> MFCC\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m mfcc = \u001b[43mextract_mfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m mfcc_list.append(mfcc)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# (B) 텍스트 -> 토큰\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mextract_mfcc\u001b[39m\u001b[34m(file_path, n_mfcc, cutoff)\u001b[39m\n\u001b[32m     25\u001b[39m n_fft = win_length\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 3) MFCC 추출\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m mfcc = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhann\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mfcc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ECE6254-Voice-Feature-Extraction/venv/lib/python3.12/site-packages/librosa/feature/spectral.py:1993\u001b[39m, in \u001b[36mmfcc\u001b[39m\u001b[34m(y, sr, S, n_mfcc, dct_type, norm, lifter, mel_norm, **kwargs)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[32m   1847\u001b[39m \n\u001b[32m   1848\u001b[39m \u001b[33;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1989\u001b[39m \u001b[33;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1992\u001b[39m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m     S = power_to_db(\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1995\u001b[39m fft = get_fftlib()\n\u001b[32m   1996\u001b[39m M: np.ndarray = fft.dct(S, axis=-\u001b[32m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m=dct_type, norm=norm)[\n\u001b[32m   1997\u001b[39m     ..., :n_mfcc, :\n\u001b[32m   1998\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ECE6254-Voice-Feature-Extraction/venv/lib/python3.12/site-packages/librosa/feature/spectral.py:2150\u001b[39m, in \u001b[36mmelspectrogram\u001b[39m\u001b[34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[39m\n\u001b[32m   2147\u001b[39m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[32m   2148\u001b[39m mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2150\u001b[39m melspec: np.ndarray = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m...ft,mf->...mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ECE6254-Voice-Feature-Extraction/venv/lib/python3.12/site-packages/numpy/_core/einsumfunc.py:1477\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(out, optimize, *operands, **kwargs)\u001b[39m\n\u001b[32m   1474\u001b[39m     right_pos.append(input_right.find(s))\n\u001b[32m   1476\u001b[39m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1477\u001b[39m new_view = \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1481\u001b[39m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result != results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ECE6254-Voice-Feature-Extraction/venv/lib/python3.12/site-packages/numpy/_core/numeric.py:1177\u001b[39m, in \u001b[36mtensordot\u001b[39m\u001b[34m(a, b, axes)\u001b[39m\n\u001b[32m   1175\u001b[39m at = a.transpose(newaxes_a).reshape(newshape_a)\n\u001b[32m   1176\u001b[39m bt = b.transpose(newaxes_b).reshape(newshape_b)\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m res = \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res.reshape(olda + oldb)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ========== 5) 실제 함수 호출 예시 ==========\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4) 메인 실행부\n",
    "# --------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # (A) Train CSV 로드 & 필터링\n",
    "    TRAIN_CSV_PATH = os.path.join(RAW_DATA_DIR, \"cv-valid-train.csv\")\n",
    "    df_train_filtered = load_and_filter_csv(TRAIN_CSV_PATH, accents=ACCENTS_OF_INTEREST)\n",
    "    print(\"Filtered train accent counts:\")\n",
    "    print(df_train_filtered['accent'].value_counts())\n",
    "\n",
    "    # (B) Train Balancing\n",
    "    df_train_balanced, info_lines = balance_by_gender_age(df_train_filtered)\n",
    "    \n",
    "    # Resample 정보 텍스트로 저장\n",
    "    txt_path = os.path.join(OUTPUT_DIR, \"train_dataset_info.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(\"Resample Factor Information:\\n\")\n",
    "        for line in info_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(\"\\nResample info saved to\", txt_path)\n",
    "\n",
    "    # (C) Balanced Train -> Valid 세트 분리\n",
    "    df_valid = split_train_valid_by_accent(df_train_balanced, df_train_filtered, valid_size=500)\n",
    "    print(\"\\nBalanced train shape:\", df_train_balanced.shape)\n",
    "    print(\"Valid shape:\", df_valid.shape)\n",
    "\n",
    "    # (D) Test CSV 로드 & 필터링 (테스트는 Balancing 생략)\n",
    "    TEST_CSV_PATH = os.path.join(RAW_DATA_DIR, \"cv-valid-test.csv\")\n",
    "    df_test_filtered = load_and_filter_csv(TEST_CSV_PATH, accents=ACCENTS_OF_INTEREST)\n",
    "    print(\"\\nFiltered test accent counts:\")\n",
    "    print(df_test_filtered['accent'].value_counts())\n",
    "\n",
    "    # (E) Label Encoding\n",
    "    #  - train_balanced의 accent로 인코더 학습 -> valid, test 동일 인코더 적용\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_train_balanced['accent_encoded'] = label_encoder.fit_transform(df_train_balanced['accent'])\n",
    "    df_valid['accent_encoded'] = label_encoder.transform(df_valid['accent'])\n",
    "    df_test_filtered['accent_encoded'] = label_encoder.transform(df_test_filtered['accent'])\n",
    "\n",
    "    # (F) CSV 저장\n",
    "    train_csv_out = os.path.join(OUTPUT_DIR, \"df_train_balanced.csv\")\n",
    "    valid_csv_out = os.path.join(OUTPUT_DIR, \"df_valid.csv\")\n",
    "    test_csv_out  = os.path.join(OUTPUT_DIR, \"df_test.csv\")\n",
    "\n",
    "    df_train_balanced.to_csv(train_csv_out, index=False)\n",
    "    df_valid.to_csv(valid_csv_out, index=False)\n",
    "    df_test_filtered.to_csv(test_csv_out, index=False)\n",
    "\n",
    "    print(\"\\nSaved balanced train to:\", train_csv_out)\n",
    "    print(\"Saved valid to:\", valid_csv_out)\n",
    "    print(\"Saved test to:\", test_csv_out)\n",
    "\n",
    "    # (G) 라벨 매핑 정보도 텍스트로 기록\n",
    "    label_map_txt = os.path.join(OUTPUT_DIR, \"label_mapping_info.txt\")\n",
    "    with open(label_map_txt, \"w\") as f:\n",
    "        f.write(\"Accent Label Mapping:\\n\")\n",
    "        for idx, accent in enumerate(label_encoder.classes_):\n",
    "            f.write(f\"{idx}: {accent}\\n\")\n",
    "    print(\"Accent label mapping saved to\", label_map_txt)\n",
    "\n",
    "BASE_DIR = '/Users/jeonsang-eon/ECE6254-Voice-Feature-Extraction/'\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, \"raw_data\")\n",
    "\n",
    "# 예: train.csv, valid.csv, test.csv 가 있다고 가정\n",
    "TRAIN_CSV_PATH = os.path.join(RAW_DATA_DIR, \"cv-other-train.csv\")\n",
    "VALID_CSV_PATH = os.path.join(RAW_DATA_DIR, \"cv-other-dev.csv\")\n",
    "TEST_CSV_PATH  = os.path.join(RAW_DATA_DIR, \"cv-other-test.csv\")\n",
    "\n",
    "# 출력 경로 설정\n",
    "output_dir = os.path.join(BASE_DIR, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1) train 세트 변환\n",
    "train_npz_path = os.path.join(output_dir, \"train_dataset.npz\")\n",
    "create_npz_from_csv(\n",
    "    csv_path=TRAIN_CSV_PATH,\n",
    "    audio_base_dir=RAW_DATA_DIR,\n",
    "    output_npz_path=train_npz_path,\n",
    "    n_mfcc=20,\n",
    "    cutoff=5000,\n",
    "    max_length_text=32,\n",
    "    time_frames_target=None,    # median을 자동으로 결정\n",
    "    label_col='accent'\n",
    ")\n",
    "\n",
    "# 2) valid 세트 변환\n",
    "valid_npz_path = os.path.join(output_dir, \"valid_dataset.npz\")\n",
    "create_npz_from_csv(\n",
    "    csv_path=VALID_CSV_PATH,\n",
    "    audio_base_dir=RAW_DATA_DIR,\n",
    "    output_npz_path=valid_npz_path,\n",
    "    n_mfcc=20,\n",
    "    cutoff=5000,\n",
    "    max_length_text=32,\n",
    "    time_frames_target=None,    # 별도로 지정 가능\n",
    "    label_col='accent'\n",
    ")\n",
    "\n",
    "# 3) test 세트 변환\n",
    "test_npz_path = os.path.join(output_dir, \"test_dataset.npz\")\n",
    "create_npz_from_csv(\n",
    "    csv_path=TEST_CSV_PATH,\n",
    "    audio_base_dir=RAW_DATA_DIR,\n",
    "    output_npz_path=test_npz_path,\n",
    "    n_mfcc=20,\n",
    "    cutoff=5000,\n",
    "    max_length_text=32,\n",
    "    time_frames_target=None,\n",
    "    label_col='accent'\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
